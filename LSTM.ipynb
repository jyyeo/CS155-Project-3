{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most of the code in this notebook is taken from https://www.kaggle.com/francescapaulin/character-level-lstm-in-pytorch/notebook\n",
    "## and modified as required\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('/data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "\n",
    "def remove_numbers(text): \n",
    "    result = re.sub(r'\\d+', '', text) \n",
    "    return result \n",
    "\n",
    "def remove_punctuation(text): \n",
    "    punctuation_set = ':' + ';' + ',' + '.' + '!' + '?' + '(' + ')'\n",
    "    translator = str.maketrans('', '', punctuation_set)\n",
    "    return text.translate(translator).replace(\"' \", \" \")\n",
    "\n",
    "\n",
    "text = remove_numbers(text_lowercase(text))\n",
    "text = text.replace(\"\\n\\n                   \\n\", \"\")\n",
    "text = text.replace(\"                   \\n\", \"\")\n",
    "text = text.replace(\"  \", \"\")\n",
    "\n",
    "seq_length =40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length, skip_size=1):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "\n",
    "#     batch_size_total = batch_size * seq_length\n",
    "    \n",
    "#     # total number of batches we can make, // integer division, round down\n",
    "#     n_batches = len(arr)//batch_size_total\n",
    "#     print(n_batches)\n",
    "    \n",
    "#     # Keep only enough characters to make full batches\n",
    "#     arr = arr[:n_batches * batch_size_total]\n",
    "#     # Reshape into batch_size rows, n. of first row is the batch size, the other lenght is inferred\n",
    "#     arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "#     # iterate through the array, one sequence at a time\n",
    "#     for n in range(0, arr.shape[1], seq_length):\n",
    "#         # The features\n",
    "#         x = arr[:, n:n+seq_length]\n",
    "#         # The targets, shifted by one\n",
    "#         y = np.zeros_like(x)\n",
    "#         try:\n",
    "#             y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "#         except IndexError:\n",
    "#             y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "#         yield x, y \n",
    "        \n",
    "    \n",
    "\n",
    "    n_batches = (len(arr)-seq_length-1) // batch_size\n",
    "    i=0\n",
    "    \n",
    "    for bb in range(n_batches):\n",
    "        \n",
    "        x = np.zeros([batch_size,seq_length],np.int64)\n",
    "        y = np.zeros([batch_size,seq_length],np.int64)\n",
    "        \n",
    "        for rr in range(batch_size):\n",
    "            x[rr,:] = arr[i:i+seq_length]\n",
    "            y[rr,:] = arr[i+1:i+seq_length+1]\n",
    "            i = i+1\n",
    "        \n",
    "        yield x ,y\n",
    "        \n",
    "# #when we call get batches we are going \n",
    "#to create a generator that iteratest through our array and returns x, y with yield command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=150, n_layers=1,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "                \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "#     val_idx = int(len(data)*(1-val_frac))\n",
    "#     data, val_data = data[:val_idx], data[val_idx:]\n",
    "#     print(data.shape)\n",
    "#     print(val_data.shape)\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        net.train()\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "        \n",
    "        net.eval()\n",
    "\n",
    "        correct = 0\n",
    "\n",
    "        # Turning off automatic differentiation\n",
    "        with torch.no_grad():\n",
    "            for x, y in get_batches(data, batch_size, seq_length):\n",
    "                x = one_hot_encode(x, n_chars)\n",
    "                inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                h = tuple([each.data for each in h])\n",
    "                output, h = net(inputs, h)\n",
    "\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
    "                correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "#         test_loss /= len(test_loader.dataset)\n",
    "            \n",
    "        \n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Acc: {:.4f}...\".format(correct/(1463*64*40)))\n",
    "            \n",
    "            # loss stats\n",
    "#             if counter % print_every == 0:\n",
    "#                 # Get validation loss\n",
    "#                 val_h = net.init_hidden(batch_size)\n",
    "#                 val_losses = []\n",
    "#                 net.eval()\n",
    "#                 for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "#                     # One-hot encode our data and make them Torch tensors\n",
    "#                     x = one_hot_encode(x, n_chars)\n",
    "#                     x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "#                     # Creating new variables for the hidden state, otherwise\n",
    "#                     # we'd backprop through the entire training history\n",
    "#                     val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "#                     inputs, targets = x, y\n",
    "#                     output, val_h = net(inputs, val_h)\n",
    "#                     val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "#                     val_losses.append(val_loss.item())\n",
    "                \n",
    "#                 net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "#                 print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "#                       \"Step: {}...\".format(counter),\n",
    "#                       \"Loss: {:.4f}...\".format(loss.item()),\n",
    "#                       \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(38, 200, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=200, out_features=38, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=200\n",
    "n_layers=1\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50... Step: 1463... Loss: 1.8565... Acc: 0.3836...\n",
      "Epoch: 2/50... Step: 2926... Loss: 1.7550... Acc: 0.4260...\n",
      "Epoch: 3/50... Step: 4389... Loss: 1.6935... Acc: 0.4547...\n",
      "Epoch: 4/50... Step: 5852... Loss: 1.6585... Acc: 0.4718...\n",
      "Epoch: 5/50... Step: 7315... Loss: 1.6591... Acc: 0.4845...\n",
      "Epoch: 6/50... Step: 8778... Loss: 1.6211... Acc: 0.4948...\n",
      "Epoch: 7/50... Step: 10241... Loss: 1.6061... Acc: 0.5041...\n",
      "Epoch: 8/50... Step: 11704... Loss: 1.6188... Acc: 0.5126...\n",
      "Epoch: 9/50... Step: 13167... Loss: 1.5871... Acc: 0.5200...\n",
      "Epoch: 10/50... Step: 14630... Loss: 1.5796... Acc: 0.5273...\n",
      "Epoch: 11/50... Step: 16093... Loss: 1.5799... Acc: 0.5334...\n",
      "Epoch: 12/50... Step: 17556... Loss: 1.5868... Acc: 0.5386...\n",
      "Epoch: 13/50... Step: 19019... Loss: 1.5791... Acc: 0.5435...\n",
      "Epoch: 14/50... Step: 20482... Loss: 1.5495... Acc: 0.5479...\n",
      "Epoch: 15/50... Step: 21945... Loss: 1.5374... Acc: 0.5517...\n",
      "Epoch: 16/50... Step: 23408... Loss: 1.5497... Acc: 0.5552...\n",
      "Epoch: 17/50... Step: 24871... Loss: 1.5403... Acc: 0.5584...\n",
      "Epoch: 18/50... Step: 26334... Loss: 1.5280... Acc: 0.5611...\n",
      "Epoch: 19/50... Step: 27797... Loss: 1.5345... Acc: 0.5631...\n",
      "Epoch: 20/50... Step: 29260... Loss: 1.4942... Acc: 0.5658...\n",
      "Epoch: 21/50... Step: 30723... Loss: 1.5079... Acc: 0.5670...\n",
      "Epoch: 22/50... Step: 32186... Loss: 1.4819... Acc: 0.5692...\n",
      "Epoch: 23/50... Step: 33649... Loss: 1.5040... Acc: 0.5709...\n",
      "Epoch: 24/50... Step: 35112... Loss: 1.4746... Acc: 0.5721...\n",
      "Epoch: 25/50... Step: 36575... Loss: 1.4922... Acc: 0.5735...\n",
      "Epoch: 26/50... Step: 38038... Loss: 1.4716... Acc: 0.5748...\n",
      "Epoch: 27/50... Step: 39501... Loss: 1.4765... Acc: 0.5764...\n",
      "Epoch: 28/50... Step: 40964... Loss: 1.4576... Acc: 0.5784...\n",
      "Epoch: 29/50... Step: 42427... Loss: 1.4450... Acc: 0.5781...\n",
      "Epoch: 30/50... Step: 43890... Loss: 1.4376... Acc: 0.5804...\n",
      "Epoch: 31/50... Step: 45353... Loss: 1.4683... Acc: 0.5818...\n",
      "Epoch: 32/50... Step: 46816... Loss: 1.4351... Acc: 0.5825...\n",
      "Epoch: 33/50... Step: 48279... Loss: 1.4513... Acc: 0.5835...\n",
      "Epoch: 34/50... Step: 49742... Loss: 1.4234... Acc: 0.5844...\n",
      "Epoch: 35/50... Step: 51205... Loss: 1.4304... Acc: 0.5850...\n",
      "Epoch: 36/50... Step: 52668... Loss: 1.4285... Acc: 0.5861...\n",
      "Epoch: 37/50... Step: 54131... Loss: 1.4153... Acc: 0.5866...\n",
      "Epoch: 38/50... Step: 55594... Loss: 1.4133... Acc: 0.5875...\n",
      "Epoch: 39/50... Step: 57057... Loss: 1.4216... Acc: 0.5878...\n",
      "Epoch: 40/50... Step: 58520... Loss: 1.4098... Acc: 0.5891...\n",
      "Epoch: 41/50... Step: 59983... Loss: 1.4180... Acc: 0.5904...\n",
      "Epoch: 42/50... Step: 61446... Loss: 1.4193... Acc: 0.5904...\n",
      "Epoch: 43/50... Step: 62909... Loss: 1.4042... Acc: 0.5919...\n",
      "Epoch: 44/50... Step: 64372... Loss: 1.4116... Acc: 0.5929...\n",
      "Epoch: 45/50... Step: 65835... Loss: 1.3920... Acc: 0.5936...\n",
      "Epoch: 46/50... Step: 67298... Loss: 1.3954... Acc: 0.5937...\n",
      "Epoch: 47/50... Step: 68761... Loss: 1.4094... Acc: 0.5945...\n",
      "Epoch: 48/50... Step: 70224... Loss: 1.3764... Acc: 0.5957...\n",
      "Epoch: 49/50... Step: 71687... Loss: 1.3795... Acc: 0.5959...\n",
      "Epoch: 50/50... Step: 73150... Loss: 1.3819... Acc: 0.5968...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_length = 40 #max length verses\n",
    "n_epochs = 50 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open('lstm_dense_50_epoch.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, T, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "\n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        # apply softmax to get p probabilities for the likely next character giving x\n",
    "        p = F.softmax(out/T, dim=1).data\n",
    "\n",
    "        # get top characters\n",
    "        # considering the k most probable characters with topk method\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, T, prime='Il', top_k=None):\n",
    "        \n",
    "    net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, T, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], T, h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "then tell my love is forget sicker eyes,\n",
      "then whoule my proud heart to so thy subject thee out.\n",
      "her by my poor not thy sweet words if vialing,\n",
      "who leaves knows use of thine be in gazed how to me with alterporion call growd thine ede him of thee, thy more be,\n",
      "that thy herst excestoned know sway,\n",
      "threw twed forth which loke pass thy rest\n",
      "a viigh and that then art a doth ever be,\n",
      "to more word aase,\n",
      "to mare are groan,\n",
      "then thou acted that i foon,\n",
      "the can my bedies ride,\n",
      "thy love part that prike,\n",
      "that that that wet the unks to me:\n",
      "thus that i see doth tround though eyes and forswild,\n",
      "in flestest of not i in u,\n",
      "doth heart per love ow overst by thy self belight she soar blessed, and bear,\n",
      "my confeet,\n",
      "and worls bitter swornst in reatures mive for bording esextrence.\n",
      "wo the liest a facollong inward have own spure of that by a manting born date.\n",
      "riving mine kniget.\n",
      "that i well love recome that,\n",
      "love of thich all my love thee fair thy fortuen best,\n",
      "that despise their oad,\n",
      "in time i brow,\n",
      "for far t\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, T = 1, prime=\"shall i compare thee to a summer's day?\\n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "thy pingured my firet thoughts, or when i am dost spet me in thee i in thy hime be olden be unforned of thy friend me wet race,\n",
      "but thou thou lov'st that be.\n",
      "so that beauty of thy fingers and in thee that stere you dost live hath placue the fairest wards these liss in one,\n",
      "and therefore was of the best and thy ingarded  bitter scanted words thou sense nd thou have the found, that heaven's grest,\n",
      "and though thy self disgrace be the sin thy fair dare my love, and thee and her forged fair last,\n",
      "who all thee thrie in and thee is not be food:\n",
      "so mo thou dost thou with me that thou shaltering thy self i form hath thou bities thou thou to mine own love sufat more thou art and wanting thy constancies when i not beauty,\n",
      "and thue i not then me doth thou thy hand,\n",
      "to love her with he proud heart that my breath i roses,\n",
      "thy dicaily in such a both sweet love in that beauty there is mine in tangle is to mine eyes doth not lease my will,\n",
      "and thy mournard of my life for their breathe me.\n",
      "a tast that in\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000,T = 0.75, prime=\"shall i compare thee to a summer's day?\\n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "the can my self a falles i restill,\n",
      "and therefore to the self art beauty still,\n",
      "and i am my love that in my love that thought to me so the thy sweet self so thoughts doth see thee,\n",
      "which in thy sweet self all thy self art thought to thee as the world is to the self-love to concest to me that the world with thy brand thee,\n",
      "when i be better that my heart to thee most be to the sun the thing thee that i am thy self art that i have sweet self to thee i be not in thy thing on thee as thou art all that i do be the sun the world thou most and thee,\n",
      "where is thy proud heart to me that the beauty of thy self thy self art in thee as the confounds,\n",
      "that i say they so that which thou thy self alone.\n",
      "the store thee and thee for my love thee that i with touch she thou thy sweet self be thee and thee,\n",
      "when i best that thou mayst in the fairest thoughts in thee,\n",
      "and therefore to my self the world and thee,\n",
      "when i bate that thou thy self alone.\n",
      "that i am thy self to the sun the contented thee all thy se\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000,T = 0.25, prime=\"shall i compare thee to a summer's day?\\n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "the world to the sun the fairest fair that thou art thoughts in thee,\n",
      "and therefore to thee my self to thee that i am forsworn,\n",
      "but thou thy self a thee,\n",
      "which in thy strong beauty speak in thee,\n",
      "and thou art to thee the world to be thee that i am forst enceeded with that which thy sweet self alone,\n",
      "that i am thy self thy self art to my self a faire,\n",
      "that i have sweet self to thee more than the world and thee,\n",
      "which thou thy self thy self art to my self a faire,\n",
      "that i have sweet self to thee more than the world and thee,\n",
      "which thou thy self thy self art to my self a faire,\n",
      "that i have sweet self to thee more than the world and thee,\n",
      "which thou thy self thy self art to my self a faire,\n",
      "that i have sweet self to thee more than the world and thee,\n",
      "which thou thy self thy self art to my self a faire,\n",
      "that i have sweet self to thee more than the world and thee,\n",
      "which thou thy self thy self art to my self a faire,\n",
      "that i have sweet self to thee more than the world and thee,\n",
      "which thou thy se\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime=\"shall i compare thee to a summer's day?\\n\",T = 1e-10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
